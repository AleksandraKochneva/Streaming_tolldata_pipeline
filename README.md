# Data Pipeline for Streaming Traffic Data

This project creates a data pipeline that collects streaming traffic data, processes it, and loads it into a MySQL database. The data is generated by a Kafka producer and consumed by a Kafka consumer, with MySQL as the storage backend.

### Files Description

- **admin.py**: A script that creates a Kafka topic using the `KafkaAdminClient`.
- **create_db_and_table.sql**: SQL script to create the database and the necessary table (`livetolldata`) for storing streaming data.
- **download_unpack_kafka.sh**: A shell script to download and extract Kafka on your local machine.
- **requirements.txt**: A file listing the necessary Python packages to run the project.
- **streaming-data-reader.py**: A Kafka consumer script that reads messages from a Kafka topic and loads them into a MySQL database.
- **toll_traffic_generator.py**: A Kafka producer script that simulates toll traffic and sends messages to a Kafka topic.

## Setup Instructions

### Prerequisites

1. **Kafka**: Make sure you have Kafka installed and running. If not, you can install it using the provided script (`download_unpack_kafka.sh`).
2. **MySQL**: You should have a MySQL instance running locally or remotely, and you must create a database named `tolldata`.

### Steps to Run the Project

1. **Download and Setup Kafka**:
    - Run the `download_unpack_kafka.sh` script to download and extract Kafka:
    
    ```bash
    bash download_unpack_kafka.sh
    ```

2. **Create Kafka Topic**:
    - Run the `admin.py` script to create a Kafka topic named `toll`:
    
    ```bash
    python admin.py
    ```

3. **Setup MySQL Database**:
    - Run the SQL script `create_db_and_table.sql` to create the `tolldata` database and the required table `livetolldata`. You can run the SQL script in your MySQL client.

4. **Install Dependencies**:
    - Install the necessary Python packages using `requirements.txt`:

    ```bash
    pip install -r requirements.txt
    ```

5. **Run the Traffic Data Generator**:
    - Start the `toll_traffic_generator.py` script to simulate traffic and send data to the Kafka topic:


6. **Run the Streaming Data Consumer**:
    - Start the `streaming-data-reader.py` script to consume data from the Kafka topic and load it into the MySQL database:


The streaming data consumer will continuously read messages from Kafka and insert them into the MySQL database.



